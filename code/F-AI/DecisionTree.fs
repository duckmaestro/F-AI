
//    This file is part of F-AI.
//
//    F-AI is free software: you can redistribute it and/or modify
//    it under the terms of the GNU Lesser General Public License as published by
//    the Free Software Foundation, either version 3 of the License, or
//    (at your option) any later version.
//
//    F-AI is distributed in the hope that it will be useful,
//    but WITHOUT ANY WARRANTY; without even the implied warranty of
//    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//    GNU Lesser General Public License for more details.
//
//    You should have received a copy of the GNU Lesser General Public License
//    along with F-AI.  If not, see <http://www.gnu.org/licenses/>.



//
//    Decision Tree learner.
//

module DecisionTree


// namespaces

open System
open MathNet
open FAI
open Classifiers


// type aliases

type private LabelCounts = System.Collections.Generic.Dictionary<int, float>


// records

type private DecisionNode = {
    FeatureId : int;
    Threshold : float;
    LabelLT : int option;
    LabelGT : int option;
    ChildLT : DecisionNode option;
    ChildGT : DecisionNode option;
}

type private CutEntropy = {
    FeatureId : int;
    Threshold : float;
    EntropyLT : float;
    EntropyGT : float;
}


// private functions
    
/// Measures the entropy of each pair of subsets
/// generated by each possible cut in the given feature
/// dimension. Entropy is approximated by size of gap
/// between maximum and minimum likelihood labels.
let private MeasureEntropyForEachCutApproximate samples feature : CutEntropy seq = 

    if samples |> Seq.isEmpty then
        Seq.empty
    else

    // sort
    let samplesSorted = 
        samples 
        |> Seq.sortBy (fun s -> s.Features.Item(feature)) 
        |> Seq.cache

    // prepare buffer for entropy values
    let mutable entropyForEachCut = new System.Collections.Generic.List<CutEntropy>()

    // sliding values
    let mutable countsByLabelLT = new LabelCounts()
    let mutable countsByLabelGT = new LabelCounts()
    
    // helper methods
    let fnIncr (counts:LabelCounts) feature = 
        let mutable currCount = 0.0
        if counts.ContainsKey(feature) then
            currCount <- counts.Item(feature)
        currCount <- currCount + 1.0
        counts.Item(feature) <- currCount
        ()
    let fnDecr (counts:LabelCounts) feature = 
        let mutable currCount = 0.0
        if counts.ContainsKey(feature) then
            currCount <- counts.Item(feature)
        currCount <- currCount - 1.0
        counts.Item(feature) <- currCount
        ()
    // approximates entropy by comparing max and min occurring labels.
    // max entropy = size of set. min entropy = 0.
    let fnEntropyApproximate (counts:LabelCounts) =
        let mutable minimumCount = System.Double.MaxValue
        let mutable maximumCount = System.Double.MinValue
        let mutable totalCount = 0.0
        for labelCount in counts do
            if labelCount.Value < minimumCount then
                minimumCount <- labelCount.Value
            if labelCount.Value > maximumCount then
                maximumCount <- labelCount.Value
            totalCount <- totalCount + labelCount.Value
        if totalCount = 0.0 then
            0.0
        else
            totalCount - (maximumCount - minimumCount)

    
    // initialize GT side
    for count in samplesSorted |> Seq.countBy (fun s -> s.Label) do
        countsByLabelGT.Item(fst count) <- float(snd count)
    
    // process middle samples
    for pair in samplesSorted |> Seq.pairwise do
        let sample1 = fst pair
        let sample2 = snd pair

        let label1 = sample1.Label
        let label2 = sample2.Label

        let value1 = sample1.Features.Item(feature)
        let value2 = sample2.Features.Item(feature)

        fnIncr countsByLabelLT label1
        fnDecr countsByLabelGT label2

        if value1 <> value2 then

            let cutLocation = (value1 + value2) * 0.5

            let entropy1 = fnEntropyApproximate countsByLabelLT
            let entropy2 = fnEntropyApproximate countsByLabelGT

            let cutEntropy = 
                { 
                    FeatureId = feature;
                    Threshold = cutLocation;
                    EntropyLT = entropy1;
                    EntropyGT = entropy2;
                }

            entropyForEachCut.Add(cutEntropy)


    // process last sample
    
    // done
    entropyForEachCut :> CutEntropy seq

/// Locates the optimal cut for a given feature, based
/// on minimizing entropy. May return None.
let private FindOptimalCutWithFeature samples featureId : CutEntropy option =
    let entropyByCut = MeasureEntropyForEachCutApproximate samples featureId

    if Seq.isEmpty entropyByCut then
        None
    else
        let optimalCut =
            entropyByCut
            |> Seq.minBy (fun cut -> cut.EntropyLT+ cut.EntropyGT)

        Some(optimalCut)

/// Finds the optimal cut, both by feature and location, minimizing entropy
/// in the resulting two subsets.
let private FindOptimalCut samples = 
    let dimensionality = samples |> Seq.head |> fun x -> x.Features.Count

    let optimalCutsByFeature = 
        [0 .. dimensionality-1]
        |> Seq.map (fun featureId -> FindOptimalCutWithFeature samples featureId)
        |> Seq.filter(fun s -> s.IsSome)
        |> Seq.map (fun s -> s.Value)

    if optimalCutsByFeature |> Seq.isEmpty then
        None
    else 
        let optimalCut = optimalCutsByFeature |> Seq.minBy (fun cut -> cut.EntropyLT + cut.EntropyGT)
        Some(optimalCut)

/// Builds the decision tree recursively. Can return None.
let rec private BuildDecisionNode samples currDepth maxDepth : DecisionNode option =
    
    // locate optimal cut
    let optimalCutAsOption = FindOptimalCut samples
    if optimalCutAsOption.IsNone then
        None
    else

        let optimalCut = optimalCutAsOption.Value

        let featureId = optimalCut.FeatureId
        let threshold = optimalCut.Threshold
    
        // subsets based on cut
        let subsetLT = 
            samples 
            |> Seq.filter (fun s -> s.Features.Item(featureId) < threshold)
            |> Seq.cache
        let subsetGT =
            samples 
            |> Seq.filter (fun s -> s.Features.Item(featureId) >= threshold)
            |> Seq.cache

        if subsetLT |> Seq.isEmpty then
            None
        elif subsetGT |> Seq.isEmpty then
            None
        else 
            // helpers
            let fnMajorityLabel samples2 =
                samples2 
                |> Seq.groupBy (fun s -> s.Label)
                |> Seq.map (fun (k,s) -> k, (Seq.length s))
                |> Seq.maxBy (fun (k,s) -> s)
                |> fun (k,s) -> k

            // are we at max depth?
            let isMaxDepth = currDepth + 1 = maxDepth

            // gather child nodes if possible
            let mutable childLT = 
                if isMaxDepth || optimalCut.EntropyLT = 0.0 then
                    None
                else
                    BuildDecisionNode subsetLT (currDepth+1) maxDepth

            let mutable childGT = 
                if isMaxDepth || optimalCut.EntropyGT = 0.0 then
                    None        
                else
                    BuildDecisionNode subsetGT (currDepth+1) maxDepth

            // gather labels if applicable
            let mutable labelLT = None
            let mutable labelGT = None

            if childLT.IsNone then
                labelLT <- Some(fnMajorityLabel subsetLT)
    
            if childGT.IsNone then
                labelGT <- Some(fnMajorityLabel subsetGT)

            // return new node
            let node = {
                FeatureId = featureId;
                Threshold = threshold;
                LabelLT = labelLT;
                LabelGT = labelGT;
                ChildLT = childLT;
                ChildGT = childGT;
            } 
            Some(node)



// types

/// Decision Tree classifier. Training occurs by locating cuts in features
/// that offer maximum information gain, according to approximate entropies.
type DecisionTreeClassifier(maxDepth) = 
    
    let mutable rootNode = { FeatureId = -1; Threshold = 0.0; LabelLT = None; LabelGT = None; ChildLT = None; ChildGT = None; }

    interface IClassifier with
        member self.Train samples =
            
            if maxDepth = 0 then failwith "Max depth must be at least 1."

            rootNode <- (BuildDecisionNode samples 0 maxDepth).Value
            
            ()

        member self.Classify sample =
            if rootNode.FeatureId = -1 then failwith "Classifier not trained."
            
            let mutable node = rootNode
            let mutable label = Option<int>.None
            while label.IsNone do
                let isGreaterThan = (sample.Features.Item(node.FeatureId) >= node.Threshold)

                if isGreaterThan then
                    if node.LabelGT.IsSome then
                        label <- node.LabelGT
                    else
                        System.Diagnostics.Debug.Assert(node.ChildGT.IsSome)
                        node <- node.ChildGT.Value
                else
                    if node.LabelLT.IsSome then
                        label <- node.LabelLT
                    else  
                        System.Diagnostics.Debug.Assert(node.ChildLT.IsSome)
                        node <- node.ChildLT.Value

            label.Value